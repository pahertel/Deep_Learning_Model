{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Integration, Natural Language Processing, Data Cleaning, Calculations and Updating SQL Database for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import panel as pn\n",
    "from datetime import date\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import simfin as sf\n",
    "\n",
    "# Load data to get API Keys from an env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Custom Functions from Python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_functions as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up API and downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API key for SimFin\n",
    "\n",
    "# # SimFin API\n",
    "# simfin_api_key = os.getenv(\"SIMFIN_API_KEY\")\n",
    "\n",
    "# SimFin free API key\n",
    "sf.set_api_key('free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the local directory where data-files are stored.\n",
    "# The directory will be created if it does not already exist.\n",
    "sf.set_data_dir('~/simfin_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"us-cashflow-ttm\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-income-ttm\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-balance-ttm\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n"
     ]
    }
   ],
   "source": [
    "# Download the data from the SimFin server and load into a Pandas DataFrame\n",
    "\n",
    "    # Cash Flow Download\n",
    "# TTM\n",
    "df_ttm_cash_flow = sf.load_cashflow(variant='ttm', market='us')\n",
    "    # Income Statement Download\n",
    "# TTM\n",
    "df_ttm_income_st = sf.load_income(variant='ttm', market='us')\n",
    "    # Balance Sheet Download\n",
    "# TTM\n",
    "df_ttm_balance_st = sf.load_balance(variant='ttm', market='us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"us-shareprices-daily\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n"
     ]
    }
   ],
   "source": [
    "    # Share Prices Download\n",
    "# Daily\n",
    "df_d_shares = sf.load_shareprices(variant='daily', market='us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"us-companies\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"industries\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"markets\" on disk (1 days old).\n",
      "- Loading from disk ... Done!\n"
     ]
    }
   ],
   "source": [
    "    # Company's IDs Download\n",
    "df_c_id = sf.load_companies(market='us')\n",
    "    # Sector/Indistry Download\n",
    "df_sect_industry = sf.load_industries()\n",
    "    # Market IDs Download\n",
    "df_m_id = sf.load_markets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily DataFrame drop columns\n",
    "price_data = df_d_shares.copy()\n",
    "price_data.drop(columns = ['SimFinId', 'Open', 'Low', 'High', 'Shares Outstanding'], inplace=True)\n",
    "\n",
    "# Second price dataframe to get around an error I was getting in the forwardfill later in the code with the merging of the prices and fundimental data\n",
    "price_data2 = price_data.copy()\n",
    "price_data2.drop(columns = ['Adj. Close', 'Dividend', 'Volume'], inplace=True)\n",
    "price_data2.rename(columns = {'Close': 'Close Delete'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTM DataFrame drop columns\n",
    "cash_flow_ttm_data = df_ttm_cash_flow.copy()\n",
    "cash_flow_ttm_data.drop(columns = ['SimFinId', 'Currency', 'Restated Date'], inplace=True)\n",
    "\n",
    "income_st_ttm_data = df_ttm_income_st.copy()\n",
    "income_st_ttm_data.drop(columns = [\n",
    "    'SimFinId', 'Currency', 'Fiscal Year', 'Fiscal Period', 'Restated Date', 'Shares (Basic)', \n",
    "    'Shares (Diluted)', 'Publish Date', 'Depreciation & Amortization'], inplace=True)\n",
    "\n",
    "balance_st_ttm_data = df_ttm_balance_st.copy()\n",
    "balance_st_ttm_data.drop(columns = [\n",
    "    'SimFinId', 'Currency', 'Fiscal Year', 'Fiscal Period', 'Restated Date', 'Shares (Basic)', \n",
    "    'Shares (Diluted)', 'Publish Date'], inplace=True)\n",
    "\n",
    "# TTM DataFrame Combine Fundimentals\n",
    "ttm_fund_data_combined = pd.concat([cash_flow_ttm_data, income_st_ttm_data, balance_st_ttm_data], axis = 'columns', join = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piotroski F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Piotroski F-score before adding in price data\n",
    "ttm_fund_data_combined['Piotroski F-score'] = cf.Piotroski_F_score(\n",
    "    ttm_fund_data_combined,\n",
    "    'Net Income (Common)',\n",
    "    'Total Assets',\n",
    "    'Net Cash from Operating Activities',\n",
    "    'Total Noncurrent Liabilities',\n",
    "    'Total Current Assets',\n",
    "    'Total Current Liabilities',\n",
    "    'Shares (Diluted)',\n",
    "    'Gross Profit',\n",
    "    'Revenue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data continued and merging data (Remove financials and real estate stocks as they are completely different and standard practice for backtesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged in Sector and Industry\n",
    "ttm_fund_data_combined = ttm_fund_data_combined.merge(df_c_id, left_index = True, right_index = True)\n",
    "ttm_fund_data_combined = ttm_fund_data_combined.join(df_sect_industry, on = 'IndustryId')\n",
    "ttm_fund_data_combined = ttm_fund_data_combined.reset_index().set_index(['Ticker', 'Publish Date'])\n",
    "\n",
    "# Remove financials and real estate stocks\n",
    "ttm_fund_data_combined = ttm_fund_data_combined[ttm_fund_data_combined['Sector'] != 'Financial Services']\n",
    "ttm_fund_data_combined = ttm_fund_data_combined[ttm_fund_data_combined['Sector'] != 'Real Estate']\n",
    "\n",
    "# Revome tickers that are not in both price and fundimentals\n",
    "fundl_tickers = ttm_fund_data_combined.index.unique(level = 0)\n",
    "price_tickers = price_data.index.unique(level = 0)\n",
    "l_func = lambda x, y: list((set(x)- set(y))) + list((set(y)- set(x)))\n",
    "unmached = l_func(fundl_tickers, price_tickers)\n",
    "clean_price_data = price_data.copy().drop(unmached, errors='ignore')\n",
    "clean_price_data2 = price_data2.copy().drop(unmached, errors='ignore')\n",
    "clean_ttm_fund_data_combined = ttm_fund_data_combined.copy().drop(unmached, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge in Price\n",
    "ttm_data_combined_price = pd.concat([clean_price_data, clean_ttm_fund_data_combined], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill fundimental data for each day of price data to calculate ratios each day\n",
    "ttm_data_combined_price_fill = ttm_data_combined_price.copy().fillna(method = 'ffill')\n",
    "\n",
    "# Drop NA\n",
    "ttm_data_all = ttm_data_combined_price_fill.copy().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTM DataFrame Combine Fundimentals & Price to get rid of all the bad pricing\n",
    "ttm_data_all = pd.concat([clean_price_data2, ttm_data_all], axis = 1)\n",
    "ttm_data_all = ttm_data_all.dropna()\n",
    "\n",
    "# Drop all the extra columns that are useless\n",
    "ttm_data_all.drop(columns = ['Close Delete', 'Dividend','Net Income/Starting Line', 'Net Income', 'SimFinId'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select to trim data to make calculations less computationaly intensive and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - Empty Database (No Trim)\n",
    "# 1 - Update Database (Trim)\n",
    "create_update_choice = 0\n",
    "if create_update_choice == 1:\n",
    "    ttm_data_all = ttm_data_all.reset_index().set_index(['level_0'])\n",
    "    ttm_data_all = ttm_data_all.loc['2018-12-31':]\n",
    "    ttm_data_all = ttm_data_all.reset_index().set_index(['level_0', 'level_1'])\n",
    "    ttm_data_all = ttm_data_all.sort_index()\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Market Cap\n",
    "ttm_data_all['Market Cap'] = ttm_data_all['Close'] * ttm_data_all['Shares (Diluted)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping some columns now to reduce the size of the dataframe\n",
    "column_single =  [\n",
    "    'Report Date', 'Fiscal Year', 'Fiscal Period', 'Shares (Basic)', 'Shares (Diluted)', 'Company Name', 'IndustryId',\n",
    "    'Non-Cash Items', 'Change in Working Capital', 'Change in Inventories', 'Change in Accounts Payable',\n",
    "    'Net Change in Long Term Investment', 'Cost of Revenue', 'Operating Expenses', 'Selling, General & Administrative',\n",
    "    'Non-Operating Income (Loss)', 'Abnormal Gains (Losses)', 'Net Extraordinary Gains (Losses)', 'Payables & Accruals',\n",
    "    'Share Capital & Additional Paid-In Capital', 'Treasury Stock', 'Retained Earnings'\n",
    "    ]\n",
    "columns = column_single\n",
    "ttm_data_all.drop(columns = columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Add % Change\n",
    "\n",
    "    # Creates a list of tickers for index values\n",
    "ticker_list = ttm_data_all.copy().reset_index().set_index(['level_0']).index.drop_duplicates()\n",
    "\n",
    "    # Create an empty dataframe that will be the combined version\n",
    "combined_df = pd.DataFrame()\n",
    "#single_change = ttm_data_all['Close'].copy()\n",
    "\n",
    "count = 0\n",
    "for ticker in ticker_list:\n",
    "    single_change = ttm_data_all[['Close']].copy().loc[[ticker]]\n",
    "    single_change['Close Chg'] = ( single_change['Close'] / single_change['Close'].shift(1) ) -1\n",
    "    \n",
    "    # add dataframes topgether\n",
    "    combined_df = combined_df.append(single_change)\n",
    "    \n",
    "    # Calculates percentage of completion\n",
    "    count = count + 1\n",
    "    percent = (count/len(ticker_list)) * 100\n",
    "    time.sleep(.01)\n",
    "    print(f\"{round(percent, 1)}%\", end=\"\\r\")\n",
    "\n",
    "\n",
    "# Add returns to main dataframe and drop N/A from percent calculation\n",
    "ttm_data_all = pd.concat([ttm_data_all, combined_df['Close Chg']], axis = 1)\n",
    "ttm_data_all = ttm_data_all.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADD UNIVERSE, SECTOR AND INDUSTRY DATA SUMS FOR COMPARIBLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the end of column name\n",
    "# 'level_0 ' = ''\n",
    "col_name_total = \" Universe\"\n",
    "col_name_sector = \" Sector\"\n",
    "col_name_industry = \" Industry\"\n",
    "\n",
    "# list to rename calculations for Stock, Total, Sector and Industry data in a for loop\n",
    "col_name_loop = ['', col_name_total, col_name_sector, col_name_industry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionaries to reduce merging time for Universe, Sector and Industry Sum\n",
    "universe_sum_df = pd.concat([ttm_data_all['Sector'], ttm_data_all['Industry']], axis = 1)\n",
    "sector_sum_df = pd.concat([ttm_data_all['Sector'], ttm_data_all['Industry']], axis = 1)\n",
    "industry_sum_df = pd.concat([ttm_data_all['Sector'], ttm_data_all['Industry']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data sum\n",
    "\n",
    "# Clean columns to get sum and column names to have the same amount of column names for Total data\n",
    "ttm_data_all_clean_all = ttm_data_all.copy().reset_index().set_index(['level_0', 'level_1'])\n",
    "ttm_data_all_clean_all.drop(columns = [\n",
    "    'Close', 'Adj. Close','Sector',\n",
    "    'Industry', 'Piotroski F-score'], inplace=True)\n",
    "\n",
    "\n",
    "# Creat Total Data to be added to main dataframe\n",
    "\n",
    "# Sum all the data\n",
    "ttm_data_all_clean_all = ttm_data_all_clean_all.copy().reset_index().groupby(['level_1']).sum()\n",
    "\n",
    "# Use col_name_change funtion to change column names with specified name at end\n",
    "ttm_data_all_clean_all.columns = cf.col_name_change(ttm_data_all_clean_all, col_name_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted Average for the Universe\n",
    "\n",
    "# Create data frame to merge with Universe before loop\n",
    "ttm_data_all_clean_all_weighted = ttm_data_all.copy().reset_index().set_index(['level_1'])\n",
    "\n",
    "# Take the market cap of the Universe and put it in the temparary Dataframe for weighted averages\n",
    "ttm_data_all_clean_all_weighted[f'Market Cap{col_name_total}'] = ttm_data_all_clean_all[f'Market Cap{col_name_total}']\n",
    "\n",
    "# Weighted Return\n",
    "ttm_data_all_clean_all_weighted[f'Weighted Return{col_name_total}'] = cf.weighted_average(\n",
    "    ttm_data_all_clean_all_weighted,\n",
    "    'Close Chg',\n",
    "    'Market Cap',\n",
    "    col_name_total)\n",
    "\n",
    "# Weighted Piotroski F-score\n",
    "ttm_data_all_clean_all_weighted[f'Weighted Piotroski F-score{col_name_total}'] = cf.weighted_average(\n",
    "    ttm_data_all_clean_all_weighted,\n",
    "    'Piotroski F-score',\n",
    "    'Market Cap',\n",
    "    col_name_total)\n",
    "\n",
    "# Sum up the \n",
    "ttm_data_all_clean_all_weighted = ttm_data_all_clean_all_weighted.groupby(['level_1']).sum()\n",
    "\n",
    "# Merge in weighted data after sum\n",
    "ttm_data_all_clean_all = pd.concat([ttm_data_all_clean_all, ttm_data_all_clean_all_weighted[f'Weighted Return{col_name_total}'], ttm_data_all_clean_all_weighted[f'Weighted Piotroski F-score{col_name_total}']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sector Data\n",
    "\n",
    "# Clean columns to get sum and column names to have the same amount of column names for Sector data\n",
    "ttm_data_all_clean_sector = ttm_data_all.copy().reset_index().set_index(['level_0', 'level_1'])\n",
    "ttm_data_all_clean_sector.drop(columns = [\n",
    "    'Close', 'Adj. Close', 'Industry',\n",
    "    'Piotroski F-score'], inplace=True)\n",
    "\n",
    "\n",
    "# Creat Sector Data to be added to main dataframe\n",
    "\n",
    "# Sum Sector data\n",
    "ttm_data_all_clean_sector = ttm_data_all_clean_sector.copy().reset_index().groupby(['Sector', 'level_1']).sum()\n",
    "\n",
    "# Use col_name_change funtion to change column names with specified name at end\n",
    "ttm_data_all_clean_sector.columns = cf.col_name_change(ttm_data_all_clean_sector, col_name_sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted Average for the Sector\n",
    "\n",
    "# Create data frame to merge with Sector before loop\n",
    "ttm_data_all_clean_sector_weighted = ttm_data_all.copy().reset_index().set_index(['Sector', 'level_1'])\n",
    "\n",
    "# Take the market cap of the Sector and put it in the temparary Dataframe for weighted averages\n",
    "ttm_data_all_clean_sector_weighted[f'Market Cap{col_name_sector}'] = ttm_data_all_clean_sector[f'Market Cap{col_name_sector}']\n",
    "\n",
    "# Weighted Return\n",
    "ttm_data_all_clean_sector_weighted[f'Weighted Return{col_name_sector}'] = cf.weighted_average(\n",
    "    ttm_data_all_clean_sector_weighted,\n",
    "    'Close Chg',\n",
    "    'Market Cap',\n",
    "    col_name_sector)\n",
    "\n",
    "# Weighted Piotroski F-score\n",
    "ttm_data_all_clean_sector_weighted[f'Weighted Piotroski F-score{col_name_sector}'] = cf.weighted_average(\n",
    "    ttm_data_all_clean_sector_weighted,\n",
    "    'Piotroski F-score',\n",
    "    'Market Cap',\n",
    "    col_name_sector)\n",
    "\n",
    "# Sum up the \n",
    "ttm_data_all_clean_sector_weighted = ttm_data_all_clean_sector_weighted.groupby(['Sector', 'level_1']).sum()\n",
    "\n",
    "# Merge in weighted data after sum\n",
    "ttm_data_all_clean_sector = pd.concat([ttm_data_all_clean_sector, ttm_data_all_clean_sector_weighted[f'Weighted Return{col_name_sector}'], ttm_data_all_clean_sector_weighted[f'Weighted Piotroski F-score{col_name_sector}']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Industry Data\n",
    "\n",
    "# Clean columns to get sum and column names to have the same amount of column names for Industry data\n",
    "ttm_data_all_clean_industry = ttm_data_all.copy().reset_index().set_index(['level_0', 'level_1'])\n",
    "ttm_data_all_clean_industry.drop(columns = [\n",
    "    'Close', 'Adj. Close','Sector',\n",
    "    'Piotroski F-score'], inplace=True)\n",
    "\n",
    "\n",
    "# Creat Industry Data to be added to main dataframe\n",
    "\n",
    "# Sum Industry data\n",
    "ttm_data_all_clean_industry = ttm_data_all_clean_industry.copy().reset_index().groupby(['Industry', 'level_1']).sum()\n",
    "\n",
    "# Use col_name_change funtion to change column names with specified name at end\n",
    "ttm_data_all_clean_industry.columns = cf.col_name_change(ttm_data_all_clean_industry, col_name_industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted Average for the Industry\n",
    "\n",
    "# Create data frame to merge with Sector before loop\n",
    "ttm_data_all_clean_industry_weighted = ttm_data_all.copy().reset_index().set_index(['Industry', 'level_1'])\n",
    "\n",
    "# Take the market cap of the Sector and put it in the temparary Dataframe for weighted averages\n",
    "ttm_data_all_clean_industry_weighted[f'Market Cap{col_name_industry}'] = ttm_data_all_clean_industry[f'Market Cap{col_name_industry}']\n",
    "\n",
    "# Weighted Return\n",
    "ttm_data_all_clean_industry_weighted[f'Weighted Return{col_name_industry}'] = cf.weighted_average(\n",
    "    ttm_data_all_clean_industry_weighted,\n",
    "    'Close Chg',\n",
    "    'Market Cap',\n",
    "    col_name_industry)\n",
    "\n",
    "# Weighted Piotroski F-score\n",
    "ttm_data_all_clean_industry_weighted[f'Weighted Piotroski F-score{col_name_industry}'] = cf.weighted_average(\n",
    "    ttm_data_all_clean_industry_weighted,\n",
    "    'Piotroski F-score',\n",
    "    'Market Cap',\n",
    "    col_name_industry)\n",
    "\n",
    "# Sum up the \n",
    "ttm_data_all_clean_industry_weighted = ttm_data_all_clean_industry_weighted.groupby(['Industry', 'level_1']).sum()\n",
    "\n",
    "# Merge in weighted data after sum\n",
    "ttm_data_all_clean_industry = pd.concat([ttm_data_all_clean_industry, ttm_data_all_clean_industry_weighted[f'Weighted Return{col_name_industry}'], ttm_data_all_clean_industry_weighted[f'Weighted Piotroski F-score{col_name_industry}']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "# Join Universe compareables to stock data\n",
    "join_stock = 'level_0'\n",
    "universe_sum_df = cf.join_obj_loop(universe_sum_df, ttm_data_all_clean_all, join_stock)\n",
    "\n",
    "# Add other daily API data here or fill monthly/qur/yearly data here to universe_sum_df\n",
    "# spy_alpaca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "# Join Sector compareables to stock data\n",
    "join_sector = 'Sector'\n",
    "sector_sum_df = cf.join_obj_loop(sector_sum_df, ttm_data_all_clean_sector, join_sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "# Join Industry compareables to stock data\n",
    "join_industry = 'Industry'\n",
    "industry_sum_df = cf.join_obj_loop(industry_sum_df, ttm_data_all_clean_industry, join_industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indexes of comparable data to get ready to merge\n",
    "universe_sum_df = universe_sum_df.reset_index().set_index(['level_0', 'level_1'])\n",
    "sector_sum_df = sector_sum_df.reset_index().set_index(['level_0', 'level_1'])\n",
    "industry_sum_df = industry_sum_df.reset_index().set_index(['level_0', 'level_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparable_data_all = pd.concat([ttm_data_all, universe_sum_df, sector_sum_df, industry_sum_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle to make a checkpoint\n",
    "comparable_data_all.to_pickle(\"Resources/pickle_file_comparable_data_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparable_data_all = pd.read_pickle(\"Resources/pickle_file_comparable_data_all.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Ratios for Stocks, Universe, Sector and Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframe to make calculations\n",
    "comparable_calculations = comparable_data_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0%\r"
     ]
    }
   ],
   "source": [
    "# CALCULATIONS (Might need to break up in different cells to run)\n",
    "\n",
    "count = 0\n",
    "for i in col_name_loop:\n",
    "    \n",
    "# Enterprise Value\n",
    "\n",
    "    # Enterprise Value (Does not include long term cash)\n",
    "\n",
    "    comparable_calculations[f\"Enterprise Value{i}\"] = cf.enterprise_value(\n",
    "        comparable_calculations,\n",
    "        f'Market Cap{i}',\n",
    "        f'Short Term Debt{i}',\n",
    "        f'Long Term Debt{i}',\n",
    "        f'Cash, Cash Equivalents & Short Term Investments{i}')\n",
    "\n",
    "# Shareholder Value\n",
    "\n",
    "    # Total Shareholder Value\n",
    "    comparable_calculations[f\"Total Shareholder Value{i}\"] = cf.shareholder_value(\n",
    "        comparable_calculations,\n",
    "        f'Cash from (Repurchase of) Equity{i}',\n",
    "        f'Cash from (Repayment of) Debt{i}',\n",
    "        f'Dividends Paid{i}')\n",
    "\n",
    "    # Shareholder Yield\n",
    "    comparable_calculations[f\"Total Shareholder Yield{i}\"] = cf.shareholder_yield(\n",
    "        comparable_calculations,\n",
    "        f'Total Shareholder Value{i}',\n",
    "        f'Market Cap{i}')\n",
    "\n",
    "    # Shareholder EV Yield\n",
    "    comparable_calculations[f\"Total Shareholder EV Yield{i}\"] = cf.shareholder_yield(\n",
    "        comparable_calculations,\n",
    "        f'Total Shareholder Value{i}',\n",
    "        f\"Enterprise Value{i}\")\n",
    "\n",
    "    # Div + Repurchase Value\n",
    "    comparable_calculations[f\"Div + Repurchase Value{i}\"] = cf.div_repurchase_value(\n",
    "        comparable_calculations,\n",
    "        f'Cash from (Repurchase of) Equity{i}',\n",
    "        f'Dividends Paid{i}')\n",
    "\n",
    "    # Div + Repurchase Yield\n",
    "    comparable_calculations[f\"Div + Repurchase Yield{i}\"] = cf.div_repurchase_yield(\n",
    "        comparable_calculations,\n",
    "        f'Div + Repurchase Value{i}',\n",
    "        f\"Market Cap{i}\")\n",
    "\n",
    "    # Div + Repurchase EV Yield\n",
    "    comparable_calculations[f\"Div + Repurchase EV Yield{i}\"] = cf.div_repurchase_yield(\n",
    "        comparable_calculations,\n",
    "        f'Div + Repurchase Value{i}',\n",
    "        f\"Enterprise Value{i}\")\n",
    "\n",
    "# P/E Ratio\n",
    "\n",
    "    # P/E (Market Cap to Earnings)\n",
    "    comparable_calculations[f\"PE Market Cap to Earnings{i}\"] = cf.pe_ratio(\n",
    "        comparable_calculations,\n",
    "        f'Market Cap{i}',\n",
    "        f'Net Income (Common){i}')\n",
    "\n",
    "    # EV/E (EV to Earnings)\n",
    "    comparable_calculations[f\"PE Market Cap to Earnings{i}\"] = cf.pe_ratio(\n",
    "        comparable_calculations,\n",
    "        f\"Enterprise Value{i}\",\n",
    "        f'Net Income (Common){i}')\n",
    "    \n",
    "# Coverage Ratios\n",
    "\n",
    "    # Earnings Coverage Ratio\n",
    "    comparable_calculations[f\"Earnings Coverage Ratio{i}\"] = cf.coverage_ratio(\n",
    "        comparable_calculations,\n",
    "        f\"Net Income (Common){i}\",\n",
    "        f'Dividends Paid{i}')\n",
    "    \n",
    "# Other Yield Ratios\n",
    "\n",
    "    # EBIT Value\n",
    "    comparable_calculations[f\"EBIT{i}\"] = cf.ebit(\n",
    "        comparable_calculations,\n",
    "        f'Net Income (Common){i}',\n",
    "        f'Interest Expense, Net{i}',\n",
    "        f'Income Tax (Expense) Benefit, Net{i}')\n",
    "\n",
    "    # EBIT Yield\n",
    "    comparable_calculations[f\"EBIT Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'EBIT{i}',\n",
    "        f'Market Cap{i}')\n",
    "\n",
    "    # EBIT EV Yield\n",
    "    comparable_calculations[f\"EBIT EV Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'EBIT{i}',\n",
    "        f\"Enterprise Value{i}\")\n",
    "\n",
    "    # EBITDA Value\n",
    "    comparable_calculations[f\"EBITDA{i}\"] = cf.ebitda(\n",
    "        comparable_calculations,\n",
    "        f'EBIT{i}',\n",
    "        f'Depreciation & Amortization{i}',)\n",
    "\n",
    "    # EBITDA Yield\n",
    "    comparable_calculations[f\"EBITDA Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'EBITDA{i}',\n",
    "        f'Market Cap{i}')\n",
    "        \n",
    "    # EBITDA EV Yield\n",
    "    comparable_calculations[f\"EBITDA EV Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'EBITDA{i}',\n",
    "        f\"Enterprise Value{i}\")\n",
    "\n",
    "    # Revenue Yield\n",
    "    comparable_calculations[f\"Revenue Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'Revenue{i}',\n",
    "        f'Market Cap{i}')\n",
    "\n",
    "    # Revenue EV Yield\n",
    "    comparable_calculations[f\"Revenue EV Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'Revenue{i}',\n",
    "        f\"Enterprise Value{i}\")\n",
    "\n",
    "    # Operating Yield\n",
    "    comparable_calculations[f\"Operating Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'Net Cash from Operating Activities{i}',\n",
    "        f'Market Cap{i}')\n",
    "\n",
    "    # Operating Ev Yield\n",
    "    comparable_calculations[f\"Operating EV Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'Net Cash from Operating Activities{i}',\n",
    "        f\"Enterprise Value{i}\")\n",
    "\n",
    "    # Cash to Market Cap Yield (Does not include long term cash & equivilents)\n",
    "    comparable_calculations[f\"Cash Yield{i}\"] = cf.yield_calc(\n",
    "        comparable_calculations,\n",
    "        f'Cash, Cash Equivalents & Short Term Investments{i}',\n",
    "        f'Market Cap{i}')\n",
    "    \n",
    "    \n",
    "    # Calculates percentage of completion\n",
    "    count = count + 1\n",
    "    percent = (count/len(col_name_loop)) * 100\n",
    "    time.sleep(.01)\n",
    "    print(f\"{round(percent, 1)}%\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data for Model Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useles features\n",
    "clean_data = comparable_calculations.copy()\n",
    "\n",
    "column_loop_sum = []\n",
    "\n",
    "for i in col_name_loop:\n",
    "    column_loop = [\n",
    "        f'Depreciation & Amortization{i}', f'Change in Accounts Receivable{i}', f'Change in Other{i}',\n",
    "        f'Net Cash from Operating Activities{i}', f'Change in Fixed Assets & Intangibles{i}',\n",
    "        f'Net Cash from Acquisitions & Divestitures{i}', f'Net Cash from Investing Activities{i}',\n",
    "        f'Dividends Paid{i}', f'Cash from (Repayment of) Debt{i}', f'Cash from (Repurchase of) Equity{i}',\n",
    "        f'Net Cash from Financing Activities{i}',f'Net Change in Cash{i}', f'Revenue{i}', f'Gross Profit{i}',\n",
    "        f'Research & Development{i}', f'Operating Income (Loss){i}', f'Interest Expense, Net{i}',\n",
    "        f'Pretax Income (Loss), Adj.{i}', f'Pretax Income (Loss){i}', f'Income Tax (Expense) Benefit, Net{i}',\n",
    "        f'Income (Loss) from Continuing Operations{i}',f'Net Income (Common){i}',\n",
    "        f'Cash, Cash Equivalents & Short Term Investments{i}', f'Accounts & Notes Receivable{i}', f'Inventories{i}',\n",
    "        f'Total Current Assets{i}', f'Property, Plant & Equipment, Net{i}', f'Long Term Investments & Receivables{i}',\n",
    "        f'Other Long Term Assets{i}', f'Total Noncurrent Assets{i}', f'Total Assets{i}', f'Short Term Debt{i}',\n",
    "        f'Total Current Liabilities{i}', f'Long Term Debt{i}', f'Total Noncurrent Liabilities{i}',\n",
    "        f'Total Liabilities{i}', f'Total Equity{i}', f'Total Liabilities & Equity{i}', f'Enterprise Value{i}',\n",
    "        f'Total Shareholder Value{i}', f'Div + Repurchase Value{i}', f'EBIT{i}', f'EBITDA{i}'\n",
    "    ]\n",
    "    column_loop_sum = column_loop_sum + column_loop\n",
    "\n",
    "columns_single = ['Yield', 'Close Chg Universe', 'Close Chg Sector', 'Close Chg Industry', 'Shareholder Value']\n",
    "\n",
    "columns = column_loop_sum + columns_single\n",
    "clean_data.drop(columns = columns, inplace=True)\n",
    "\n",
    "clean_data['Close Chg'] = clean_data['Close Chg'].round(6)\n",
    "clean_data['Weighted Return Universe'] = clean_data['Weighted Return Universe'].round(6)\n",
    "clean_data['Weighted Return Sector'] = clean_data['Weighted Return Sector'].round(6)\n",
    "clean_data['Weighted Return Industry'] = clean_data['Weighted Return Industry'].round(6)\n",
    "\n",
    "clean_data = clean_data.loc[:,~clean_data.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.read_pickle(\"Resources/clean_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_aapl = clean_data.copy().loc[['AAPL']]\n",
    "clean_data_aapl = clean_data_aapl.reset_index().set_index(['level_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Fundamental Data in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "\n",
    "# Local Host Postgre SQL Number\n",
    "sql_post_api = os.getenv(\"POSTGRES_SQL_API_KEY\")\n",
    "\n",
    "# Define the database\n",
    "db_name = 'stock_market_price_fundimental_db'\n",
    "\n",
    "# Define the database URL\n",
    "db_url = f\"{sql_post_api}{db_name}\"\n",
    "\n",
    "# Create the engine object\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Table name\n",
    "main_fundamentals_table_name = 'aapl_fundamental_data'\n",
    "temp_fundamentals = 'temp_aapl_fundamental_data'\n",
    "temp_main_fundamentals = 'temp_aapl_fundamnetal_db'\n",
    "\n",
    "# Temparary SQL Table\n",
    "clean_data_aapl.to_sql(temp_fundamentals, engine, if_exists = 'replace')\n",
    "\n",
    "# Combine the new API request data with the main SQL database\n",
    "query = f\"\"\"\n",
    "\n",
    "SELECT * INTO {temp_main_fundamentals} FROM {temp_fundamentals}\n",
    "UNION\n",
    "SELECT * FROM {main_fundamentals_table_name};\n",
    "\n",
    "SELECT *\n",
    "FROM {temp_main_fundamentals};\n",
    "\"\"\"\n",
    "\n",
    "# Save and Update SQL database\n",
    "temp_fundamental_db = pd.read_sql(query, engine)\n",
    "temp_fundamental_db.to_sql(main_fundamentals_table_name, engine, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_1</th>\n",
       "      <th>level_0</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Piotroski F-score</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Market Cap</th>\n",
       "      <th>Close Chg</th>\n",
       "      <th>...</th>\n",
       "      <th>Earnings Coverage Ratio Industry</th>\n",
       "      <th>EBIT Yield Industry</th>\n",
       "      <th>EBIT EV Yield Industry</th>\n",
       "      <th>EBITDA Yield Industry</th>\n",
       "      <th>EBITDA EV Yield Industry</th>\n",
       "      <th>Revenue Yield Industry</th>\n",
       "      <th>Revenue EV Yield Industry</th>\n",
       "      <th>Operating Yield Industry</th>\n",
       "      <th>Operating EV Yield Industry</th>\n",
       "      <th>Cash Yield Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>69.23</td>\n",
       "      <td>68.70</td>\n",
       "      <td>32503750.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Hardware</td>\n",
       "      <td>1.265317e+12</td>\n",
       "      <td>-0.020792</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861081</td>\n",
       "      <td>5.451792</td>\n",
       "      <td>5.033496</td>\n",
       "      <td>7.415575</td>\n",
       "      <td>6.846604</td>\n",
       "      <td>44.265124</td>\n",
       "      <td>40.868819</td>\n",
       "      <td>6.899867</td>\n",
       "      <td>6.370465</td>\n",
       "      <td>9.611299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3346</th>\n",
       "      <td>2020-04-21</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>67.09</td>\n",
       "      <td>66.58</td>\n",
       "      <td>45247893.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Hardware</td>\n",
       "      <td>1.226204e+12</td>\n",
       "      <td>-0.030911</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861081</td>\n",
       "      <td>5.639101</td>\n",
       "      <td>5.192744</td>\n",
       "      <td>7.670353</td>\n",
       "      <td>7.063215</td>\n",
       "      <td>45.785948</td>\n",
       "      <td>42.161813</td>\n",
       "      <td>7.136927</td>\n",
       "      <td>6.572012</td>\n",
       "      <td>9.941516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>69.03</td>\n",
       "      <td>68.50</td>\n",
       "      <td>29264342.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Hardware</td>\n",
       "      <td>1.261662e+12</td>\n",
       "      <td>0.028916</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861101</td>\n",
       "      <td>5.475645</td>\n",
       "      <td>5.053864</td>\n",
       "      <td>7.448017</td>\n",
       "      <td>6.874307</td>\n",
       "      <td>44.458676</td>\n",
       "      <td>41.034089</td>\n",
       "      <td>6.930610</td>\n",
       "      <td>6.396755</td>\n",
       "      <td>9.653438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3348</th>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>68.76</td>\n",
       "      <td>68.23</td>\n",
       "      <td>31203582.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Hardware</td>\n",
       "      <td>1.256727e+12</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861101</td>\n",
       "      <td>5.486392</td>\n",
       "      <td>5.063018</td>\n",
       "      <td>7.462635</td>\n",
       "      <td>6.886758</td>\n",
       "      <td>44.545935</td>\n",
       "      <td>41.108411</td>\n",
       "      <td>6.944212</td>\n",
       "      <td>6.408341</td>\n",
       "      <td>9.672385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3349</th>\n",
       "      <td>2020-04-24</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>70.74</td>\n",
       "      <td>70.20</td>\n",
       "      <td>31627183.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Computer Hardware</td>\n",
       "      <td>1.292915e+12</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.860215</td>\n",
       "      <td>5.334682</td>\n",
       "      <td>4.933260</td>\n",
       "      <td>7.256769</td>\n",
       "      <td>6.710714</td>\n",
       "      <td>43.337829</td>\n",
       "      <td>40.076759</td>\n",
       "      <td>6.759268</td>\n",
       "      <td>6.250649</td>\n",
       "      <td>9.500935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        level_1 level_0  Close  Adj. Close      Volume  Piotroski F-score  \\\n",
       "3345 2020-04-20    AAPL  69.23       68.70  32503750.0                6.0   \n",
       "3346 2020-04-21    AAPL  67.09       66.58  45247893.0                6.0   \n",
       "3347 2020-04-22    AAPL  69.03       68.50  29264342.0                6.0   \n",
       "3348 2020-04-23    AAPL  68.76       68.23  31203582.0                6.0   \n",
       "3349 2020-04-24    AAPL  70.74       70.20  31627183.0                6.0   \n",
       "\n",
       "          Sector           Industry    Market Cap  Close Chg  ...  \\\n",
       "3345  Technology  Computer Hardware  1.265317e+12  -0.020792  ...   \n",
       "3346  Technology  Computer Hardware  1.226204e+12  -0.030911  ...   \n",
       "3347  Technology  Computer Hardware  1.261662e+12   0.028916  ...   \n",
       "3348  Technology  Computer Hardware  1.256727e+12  -0.003911  ...   \n",
       "3349  Technology  Computer Hardware  1.292915e+12   0.028796  ...   \n",
       "\n",
       "      Earnings Coverage Ratio Industry  EBIT Yield Industry  \\\n",
       "3345                          1.861081             5.451792   \n",
       "3346                          1.861081             5.639101   \n",
       "3347                          1.861101             5.475645   \n",
       "3348                          1.861101             5.486392   \n",
       "3349                          1.860215             5.334682   \n",
       "\n",
       "      EBIT EV Yield Industry  EBITDA Yield Industry  EBITDA EV Yield Industry  \\\n",
       "3345                5.033496               7.415575                  6.846604   \n",
       "3346                5.192744               7.670353                  7.063215   \n",
       "3347                5.053864               7.448017                  6.874307   \n",
       "3348                5.063018               7.462635                  6.886758   \n",
       "3349                4.933260               7.256769                  6.710714   \n",
       "\n",
       "      Revenue Yield Industry  Revenue EV Yield Industry  \\\n",
       "3345               44.265124                  40.868819   \n",
       "3346               45.785948                  42.161813   \n",
       "3347               44.458676                  41.034089   \n",
       "3348               44.545935                  41.108411   \n",
       "3349               43.337829                  40.076759   \n",
       "\n",
       "      Operating Yield Industry  Operating EV Yield Industry  \\\n",
       "3345                  6.899867                     6.370465   \n",
       "3346                  7.136927                     6.572012   \n",
       "3347                  6.930610                     6.396755   \n",
       "3348                  6.944212                     6.408341   \n",
       "3349                  6.759268                     6.250649   \n",
       "\n",
       "      Cash Yield Industry  \n",
       "3345             9.611299  \n",
       "3346             9.941516  \n",
       "3347             9.653438  \n",
       "3348             9.672385  \n",
       "3349             9.500935  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_fundamental_db.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Proccessing Database update and IBM Watson Tone Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles for selected days\n",
    "start_date = datetime.date(2017, 12, 2)\n",
    "end_date = datetime.date(2018, 3, 1)\n",
    "delta = datetime.timedelta(days=1)\n",
    "articles=[]\n",
    "while start_date <= end_date:\n",
    "    gnews_api = os.getenv(\"gnews_api\")\n",
    "    gnews_url =f\"https://gnews.io/api/v4/search?q=apple&in=finance&from={start_date}T00:01:36Z&to={start_date}T23:59:36Z&lang=en&token={gnews_api}\"\n",
    "    response = requests.get(gnews_url)\n",
    "    data = response.json()\n",
    "    articles.append(data)\n",
    "    start_date += delta\n",
    "    # pause for api restrictions\n",
    "    time.sleep(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe from JSON file with publish date, title and description\n",
    "articles_df = pd.json_normalize(articles, record_path = ['articles'], meta = 'totalArticles')\n",
    "articles_df['title&description']= articles_df['title'] + \" \" + articles_df['description']\n",
    "articles_df = articles_df[['publishedAt', 'title&description', 'totalArticles']]\n",
    "articles_df.rename(columns = {'publishedAt':'date'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Dataframe and group publish date\n",
    "articles_df['date'] = pd.to_datetime(articles_df['date'], infer_datetime_format=True).dt.date\n",
    "aapl_articles = articles_df.groupby(by = [\"date\",'totalArticles']).sum()\n",
    "aapl_articles = aapl_articles.copy().reset_index().set_index(['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalArticles</th>\n",
       "      <th>title&amp;description</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>1</td>\n",
       "      <td>Chinese Report Expects 11-Inch iPad Pro at WWD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>1</td>\n",
       "      <td>Steve Jobs Employment Questionnaire Sold for $...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>1</td>\n",
       "      <td>Android P feature spotlight: Apps built for An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>1</td>\n",
       "      <td>New Low-Cost 9.7-Inch iPad May Support Apple P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple CEO Tim Cook Calls for Stronger Privacy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            totalArticles                                  title&description\n",
       "date                                                                        \n",
       "2018-03-13              1  Chinese Report Expects 11-Inch iPad Pro at WWD...\n",
       "2018-03-16              1  Steve Jobs Employment Questionnaire Sold for $...\n",
       "2018-03-18              1  Android P feature spotlight: Apps built for An...\n",
       "2018-03-23              1  New Low-Cost 9.7-Inch iPad May Support Apple P...\n",
       "2018-03-24              1  Apple CEO Tim Cook Calls for Stronger Privacy ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM Watson Tone Analysis of AAPL articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IBM Watson\n",
    "from ibm_watson import ToneAnalyzerV3\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "# IBM Watson API Key\n",
    "ibm_watson_api_key = os.getenv(\"IBM_WATSON_API_KEY\")\n",
    "authenticator = IAMAuthenticator(ibm_watson_api_key)\n",
    "tone_analyzer = ToneAnalyzerV3(\n",
    "    version ='2017-09-21',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "# UBM Watson URL\n",
    "tone_analyzer.set_service_url('https://api.us-south.tone-analyzer.watson.cloud.ibm.com')\n",
    "tone_analyzer.set_disable_ssl_verification(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "C:\\Users\\palex\\.conda\\envs\\pyvizenv\\lib\\site-packages\\urllib3\\connectionpool.py:988: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.us-south.tone-analyzer.watson.cloud.ibm.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "# Analyzing the tones in each article\n",
    "tone_analyzed = []\n",
    "for text in aapl_articles[\"title&description\"]:\n",
    "    tone_analysis = tone_analyzer.tone(\n",
    "    {'text': text},\n",
    "    content_type ='application/json').get_result()\n",
    "    tone_analyzed.append(tone_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tones into a dataframe\n",
    "tone = []\n",
    "for text in tone_analyzed:\n",
    "    dic = {}\n",
    "    tone.append(dic)\n",
    "    for emotions in text[\"document_tone\"][\"tones\"]:\n",
    "        dic.update({emotions['tone_id']:emotions['score']})\n",
    "aapl_tone_analysis = pd.DataFrame(tone)\n",
    "aapl_tone_analysis.fillna(0, inplace = True)\n",
    "\n",
    "# Combining the tone analysis dataframe with the article dataframe\n",
    "aapl_articles.reset_index(inplace = True)\n",
    "aapl_nlp = aapl_articles.join(aapl_tone_analysis)\n",
    "aapl_nlp.set_index(\"date\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalArticles</th>\n",
       "      <th>title&amp;description</th>\n",
       "      <th>analytical</th>\n",
       "      <th>tentative</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-03-13</th>\n",
       "      <td>1</td>\n",
       "      <td>Chinese Report Expects 11-Inch iPad Pro at WWD...</td>\n",
       "      <td>0.881595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-16</th>\n",
       "      <td>1</td>\n",
       "      <td>Steve Jobs Employment Questionnaire Sold for $...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-18</th>\n",
       "      <td>1</td>\n",
       "      <td>Android P feature spotlight: Apps built for An...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.672810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-23</th>\n",
       "      <td>1</td>\n",
       "      <td>New Low-Cost 9.7-Inch iPad May Support Apple P...</td>\n",
       "      <td>0.671030</td>\n",
       "      <td>0.599484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-24</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple CEO Tim Cook Calls for Stronger Privacy ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.558892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            totalArticles                                  title&description  \\\n",
       "date                                                                           \n",
       "2018-03-13              1  Chinese Report Expects 11-Inch iPad Pro at WWD...   \n",
       "2018-03-16              1  Steve Jobs Employment Questionnaire Sold for $...   \n",
       "2018-03-18              1  Android P feature spotlight: Apps built for An...   \n",
       "2018-03-23              1  New Low-Cost 9.7-Inch iPad May Support Apple P...   \n",
       "2018-03-24              1  Apple CEO Tim Cook Calls for Stronger Privacy ...   \n",
       "\n",
       "            analytical  tentative       joy  sadness  \n",
       "date                                                  \n",
       "2018-03-13    0.881595   0.000000  0.000000      0.0  \n",
       "2018-03-16    0.000000   0.000000  0.000000      0.0  \n",
       "2018-03-18    0.000000   0.672810  0.000000      0.0  \n",
       "2018-03-23    0.671030   0.599484  0.000000      0.0  \n",
       "2018-03-24    0.000000   0.000000  0.558892      0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aapl_nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Data in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import sqlalchemy\n",
    "\n",
    "# Local Host Postgre SQL Number\n",
    "sql_post_api = os.getenv(\"POSTGRES_SQL_API_KEY\")\n",
    "\n",
    "# Define the database\n",
    "db_name = 'stock_market_price_fundimental_db'\n",
    "\n",
    "# Define the database URL\n",
    "db_url = f\"{sql_post_api}{db_name}\"\n",
    "\n",
    "# Create the engine object\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Table name\n",
    "main_nlp_table_name = 'aapl_nlp'\n",
    "temp_nlp = 'temp_aapl_nlp_data'\n",
    "temp_main_nlp = 'temp_aapl_nlp_db'\n",
    "\n",
    "# Temparary SQL Table\n",
    "aapl_nlp.to_sql(temp_nlp, engine, if_exists = 'replace')\n",
    "\n",
    "# Combine the new API request data with the main SQL database\n",
    "query = f\"\"\"\n",
    "\n",
    "SELECT * INTO {temp_main_nlp} FROM {temp_nlp}\n",
    "UNION\n",
    "SELECT * FROM {main_nlp_table_name};\n",
    "\n",
    "SELECT *\n",
    "FROM {temp_main_nlp};\n",
    "\"\"\"\n",
    "\n",
    "# Save and Update SQL database\n",
    "temp_nlp_db = pd.read_sql(query, engine)\n",
    "temp_nlp_db.to_sql(main_nlp_table_name, engine, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-pyvizenv] *",
   "language": "python",
   "name": "conda-env-.conda-pyvizenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
